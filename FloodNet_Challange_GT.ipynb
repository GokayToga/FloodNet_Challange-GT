{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1155yzdnFv6vpssblRstk8hXRo4oe2KXQ",
      "authorship_tag": "ABX9TyPgauozbdbAlXzBlM3c7rPS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GokayToga/FloodNet_Challange-GT/blob/main/FloodNet_Challange_GT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# To Do:\n",
        "\n",
        "*   resize the images to 713x713 ✅\n",
        "*   Augment the images with random shuffling, scaling, flipping, and random rotation ✅\n",
        "*  First Build basic U-net to experiment\n",
        "*  Then PSPNet, ENet, and DeepLabv3+ can be implemented and experimented on.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8UytJyo4Pyft"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOz81OFzaDTz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c6bd93a-07f8-43ae-84fb-d136a0456cb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Libraries**"
      ],
      "metadata": {
        "id": "o8JiYotYeutr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import imageio\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, f1_score\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from collections import Counter\n",
        "\n",
        "from tensorflow.image import resize as tf_resize\n",
        "from tensorflow import io as tf_io\n",
        "from tensorflow import image as tf_image\n",
        "\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
      ],
      "metadata": {
        "id": "bn1rqjfLe6FV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install tqdm"
      ],
      "metadata": {
        "id": "XaVyU-2FJllM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Resizing and Augmentation"
      ],
      "metadata": {
        "id": "leLPRMOBQxe3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Data_struct = \"\"\"\n",
        "|\n",
        "│   class_mapping.csv\n",
        "│\n",
        "└──>Test\n",
        "│   │\n",
        "│   └──>Labeled img (.png)\n",
        "│       │\n",
        "│       └──> images\n",
        "│   │\n",
        "│   └──>Unlabeled img (.jpg)\n",
        "│       │\n",
        "│       └──> images\n",
        "└──>Train\n",
        "│   │\n",
        "│   └──>Labeled img (.png)\n",
        "│       │\n",
        "│       └──> images\n",
        "│   │\n",
        "│   └──>Unlabeled img (.jpg)\n",
        "│       │\n",
        "│       └──> images\n",
        "└──>val\n",
        "    │\n",
        "    └──>Labeled img (.png)\n",
        "        │\n",
        "        └──> images\n",
        "    │\n",
        "    └──>Unlabeled img (.jpg)\n",
        "        │\n",
        "        └──> images\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "j7TwiVmDeEXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RESIZE = (713,713) #shape of resized image\n",
        "drive_path = \"/content/drive/MyDrive/FloodNet-Supervised_v1.0\"\n",
        "local_path = \"/content/Augmented_Data\"\n",
        "\n",
        "def Augment_Resize(path, resize=RESIZE):\n",
        "  #used_path = os.path.join(drive_path, path) #path for diffent datasets\n",
        "  used_path = drive_path + path\n",
        "  save_format = os.path.splitext(used_path)[1].lower()\n",
        "\n",
        "  Aug_parameters = ImageDataGenerator(   #augmentation parameters\n",
        "      rescale=1./255,          # Scaling\n",
        "      horizontal_flip=True,    # Random horizontal flip\n",
        "      vertical_flip=True,      # Random vertical flip\n",
        "      rotation_range=45        # Random rotation in the range [-45, 45] degrees\n",
        "  )\n",
        "\n",
        "  if len(os.listdir(used_path)) > 1:\n",
        "\n",
        "    length = len(os.listdir(used_path))\n",
        "    #print( length )\n",
        "\n",
        "    for img_name in tqdm(os.listdir(used_path)):\n",
        "      img_path = used_path + img_name\n",
        "      # Read and resize the image\n",
        "      img = cv2.imread(img_path)\n",
        "\n",
        "      # Check if the image was read successfully\n",
        "      if img is not None:\n",
        "        img = cv2.resize(img, resize)\n",
        "\n",
        "        # Reshape to meet the requirements of ImageDataGenerator\n",
        "        img = img.reshape((1,) + img.shape)\n",
        "\n",
        "        # Generate augmented images\n",
        "        for batch in Aug_parameters.flow(img, batch_size=1, save_to_dir=local_path, save_prefix=img_name.split('.')[0], save_format=save_format):\n",
        "          break  # Stop after one augmented image (to avoid infinite loop)\n",
        "      else:\n",
        "        print(f\"Error: Unable to read image at {img_path}\")\n",
        "  else:\n",
        "    print(f\"{path}  images are already saved\")\n"
      ],
      "metadata": {
        "id": "apMuV-t0RVVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data resizer :\n",
        "- (Because of the problems in augmentation we countinue with only resizing)"
      ],
      "metadata": {
        "id": "53tzC3cFge49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "RESIZE = (713,713) #shape of resized image\n",
        "drive_path = \"/content/drive/MyDrive/FloodNet-Supervised_v1.0\"\n",
        "local_path = \"/content/Augmented_Data\"\n",
        "\n",
        "def Augment_Resize2(path, resize=RESIZE):\n",
        "  used_path = drive_path + path\n",
        "  save_format = os.path.splitext(used_path)[1].lower()\n",
        "\n",
        "\n",
        "  if len(os.listdir(used_path)) > 1:\n",
        "\n",
        "    length = len(os.listdir(used_path))\n",
        "\n",
        "    for img_name in tqdm(os.listdir(used_path)):\n",
        "      img_path = used_path + \"/\" + img_name\n",
        "      save_path = local_path + path + \"/\" + img_name\n",
        "\n",
        "      try:\n",
        "          img = imageio.imread(img_path)\n",
        "      except Exception as e:\n",
        "          print(f\"Error: {e}\")\n",
        "          continue\n",
        "\n",
        "      # Check if the image was read successfully\n",
        "      if img is not None:\n",
        "\n",
        "        img = cv2.resize(img, resize)\n",
        "        imageio.imwrite(save_path, img)\n",
        "\n",
        "      else:\n",
        "        print(f\"Error: Unable to read image at {img_path}\")\n",
        "  else:\n",
        "    print(f\"{path}  images are already saved\")\n"
      ],
      "metadata": {
        "id": "mcijjIOdS3ep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test\n",
        "Augment_Resize2(\"/test/test-label-img\")\n",
        "Augment_Resize2(\"/test/test-org-img\")\n",
        "\n",
        "#Train\n",
        "Augment_Resize2(\"/train/train-label-img\")\n",
        "Augment_Resize2(\"/train/train-org-img\")\n",
        "\n",
        "#Val\n",
        "Augment_Resize2(\"/val/val-label-img\")\n",
        "Augment_Resize2(\"/val/val-org-img\")\n",
        "\n"
      ],
      "metadata": {
        "id": "QLXAdoiSYwdB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f38c5b70-8114-4c3a-f211-06466deb1237"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/448 [00:00<?, ?it/s]<ipython-input-3-d434104c1896>:22: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
            "  img = imageio.imread(img_path)\n",
            "100%|██████████| 448/448 [00:46<00:00,  9.62it/s]\n",
            "100%|██████████| 448/448 [02:36<00:00,  2.87it/s]\n",
            "100%|██████████| 1445/1445 [02:25<00:00,  9.95it/s]\n",
            "100%|██████████| 1445/1445 [09:49<00:00,  2.45it/s]\n",
            "100%|██████████| 450/450 [00:49<00:00,  9.02it/s]\n",
            "100%|██████████| 450/450 [02:45<00:00,  2.73it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-Moving the data to Drive"
      ],
      "metadata": {
        "id": "_3tXkZ8FL2xK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/Augmented_Data /content/drive/MyDrive\n"
      ],
      "metadata": {
        "id": "HxSobXXyLyTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-Downloading the data"
      ],
      "metadata": {
        "id": "A3buYfWEL8bD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/drive/MyDrive/Augmented_Data')"
      ],
      "metadata": {
        "id": "RR20SM8cL_gE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transforming and Augmenting Data"
      ],
      "metadata": {
        "id": "NWlA0wXNg4iE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEGMENTS = {'Background':0,'Building-flooded':1,'Building-non-flooded':2,'Road-flooded':3,'Road-non-flooded':4,\n",
        "         'Water':5,'Tree':6,'Vehicle':7,'Pool':8,'Grass':9}\n",
        "DIMENSIONS = (713,713)"
      ],
      "metadata": {
        "id": "tic7FyqWIiJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "Aug_parameters = ImageDataGenerator(   #augmentation parameters\n",
        "      rescale=1./255,          # Scaling\n",
        "      horizontal_flip=True,    # Random horizontal flip\n",
        "      vertical_flip=True,      # Random vertical flip\n",
        "      rotation_range=45        # Random rotation in the range [-45, 45] degrees\n",
        "  )"
      ],
      "metadata": {
        "id": "cIQSFpNcXiO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class SegDataset(tf.keras.utils.Sequence):\n",
        "    def __init__(self, x_paths, y_labels, batch_size, transform=None):\n",
        "        self.x_paths = x_paths\n",
        "        self.y_labels = y_labels\n",
        "        self.batch_size = batch_size\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x_paths) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        batch_x_paths = self.x_paths[index * self.batch_size: (index + 1) * self.batch_size]\n",
        "        batch_y_labels = self.y_labels[index * self.batch_size: (index + 1) * self.batch_size]\n",
        "\n",
        "        batch_images = [cv2.imread(path) for path in batch_x_paths]\n",
        "        batch_labels = batch_y_labels\n",
        "\n",
        "        if self.transform:\n",
        "            batch_images = self.transform.flow(batch_images, batch_size=len(batch_images), shuffle=False)\n",
        "\n",
        "        batch_images = [cv2.resize(img, DIMENSIONS) / 255.0 for img in batch_images]\n",
        "\n",
        "        return tf.convert_to_tensor(batch_images, dtype=tf.float32), tf.convert_to_tensor(batch_labels, dtype=tf.int64)\n",
        "\n",
        "class SegDataset3(tf.keras.utils.Sequence):#USED\n",
        "    def __init__(self, x_paths, y_labels, batch_size, transform=None):\n",
        "        self.x_paths = x_paths\n",
        "        self.y_labels = y_labels\n",
        "        self.batch_size = batch_size\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x_paths) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        batch_x_paths = self.x_paths[index * self.batch_size: (index + 1) * self.batch_size]\n",
        "        batch_y_labels = self.y_labels[index * self.batch_size: (index + 1) * self.batch_size]\n",
        "\n",
        "        batch_images = [cv2.imread(path) for path in batch_x_paths]\n",
        "        batch_masks = [cv2.imread(path, cv2.IMREAD_GRAYSCALE) for path in batch_y_labels]\n",
        "\n",
        "        if self.transform:\n",
        "            # Create an instance of ImageDataGenerator and apply the flow method\n",
        "            datagen = self.transform.flow(np.array(batch_images), batch_size=len(batch_images), shuffle=False)\n",
        "            augmented_images = datagen.next()\n",
        "\n",
        "            augmented_masks = np.array(batch_masks)\n",
        "\n",
        "            batch_images = augmented_images\n",
        "            batch_masks = augmented_masks\n",
        "\n",
        "        #batch_images = [cv2.resize(img, DIMENSIONS) / 255.0 for img in batch_images]\n",
        "        batch_images = [cv2.resize(img.astype(np.uint8), DIMENSIONS) / 255.0 for img in batch_images]\n",
        "        #batch_masks = [cv2.resize(mask, DIMENSIONS, interpolation=cv2.INTER_NEAREST) for mask in batch_masks]\n",
        "        batch_masks = [cv2.resize(mask.astype(np.uint8), DIMENSIONS, interpolation=cv2.INTER_NEAREST) for mask in batch_masks]\n",
        "\n",
        "\n",
        "        return tf.convert_to_tensor(batch_images, dtype=tf.float32), tf.convert_to_tensor(batch_masks, dtype=tf.int64)\n",
        "\n",
        "\n",
        "class SegDataset4(tf.keras.utils.Sequence):#for resizing if needed for training performance\n",
        "    def __init__(self, x_paths, y_labels, batch_size, transform=None, target_size=(256, 256)):\n",
        "        self.x_paths = x_paths\n",
        "        self.y_labels = y_labels\n",
        "        self.batch_size = batch_size\n",
        "        self.transform = transform\n",
        "        self.target_size = target_size  # Set  desired target size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x_paths) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        batch_x_paths = self.x_paths[index * self.batch_size: (index + 1) * self.batch_size]\n",
        "        batch_y_labels = self.y_labels[index * self.batch_size: (index + 1) * self.batch_size]\n",
        "\n",
        "        batch_images = [cv2.imread(path) for path in batch_x_paths]\n",
        "        batch_masks = [cv2.imread(path, cv2.IMREAD_GRAYSCALE) for path in batch_y_labels]\n",
        "\n",
        "        # Resize images and masks\n",
        "        batch_images = [cv2.resize(img, self.target_size) / 255.0 for img in batch_images]\n",
        "        batch_masks = [cv2.resize(mask, self.target_size, interpolation=cv2.INTER_NEAREST) for mask in batch_masks]\n",
        "\n",
        "        if self.transform:\n",
        "            # Create an instance of ImageDataGenerator and apply the flow method\n",
        "            datagen = self.transform.flow(np.array(batch_images), batch_size=len(batch_images), shuffle=False)\n",
        "            augmented_images = datagen.next()\n",
        "\n",
        "            augmented_masks = np.array(batch_masks)\n",
        "\n",
        "            batch_images = augmented_images\n",
        "            batch_masks = augmented_masks\n",
        "\n",
        "        return tf.convert_to_tensor(batch_images, dtype=tf.float32), tf.convert_to_tensor(batch_masks, dtype=tf.int64)\n",
        "\n",
        "\n",
        "#one hot encoding for later use\n",
        "class SegDataset2(tf.keras.utils.Sequence):\n",
        "    def __init__(self, x_paths, y_labels, batch_size, num_classes = 10, transform=None):\n",
        "        self.x_paths = x_paths\n",
        "        self.y_labels = y_labels\n",
        "        self.batch_size = batch_size\n",
        "        self.num_classes = num_classes\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x_paths) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        batch_x_paths = self.x_paths[index * self.batch_size: (index + 1) * self.batch_size]\n",
        "        batch_y_labels = self.y_labels[index * self.batch_size: (index + 1) * self.batch_size]\n",
        "\n",
        "        batch_images = [cv2.imread(path) for path in batch_x_paths]\n",
        "        batch_labels = np.array(batch_y_labels)  # Convert to numpy array for easier manipulation\n",
        "\n",
        "        if self.transform:\n",
        "            batch_images = self.transform.flow(batch_images, batch_size=len(batch_images), shuffle=False)\n",
        "\n",
        "        batch_images = [cv2.resize(img, DIMENSIONS) / 255.0 for img in batch_images]\n",
        "\n",
        "        # One-hot encode the labels\n",
        "        batch_labels_one_hot = tf.one_hot(batch_labels, self.num_classes)\n",
        "\n",
        "        return tf.convert_to_tensor(batch_images, dtype=tf.float32), tf.convert_to_tensor(batch_labels_one_hot, dtype=tf.float32)"
      ],
      "metadata": {
        "id": "KSF0oJPiZ3mn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data pathways or sm\n",
        "#path : /content/drive/MyDrive/Augmented_Data\n",
        "#specific path : /content/drive/MyDrive/Augmented_Data/test/test-label-img"
      ],
      "metadata": {
        "id": "ljwOhFUoWu7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model"
      ],
      "metadata": {
        "id": "nqGQtnRUWwDw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install --upgrade tensorflow"
      ],
      "metadata": {
        "id": "Pf6HHoedOp6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   First Configuration\n",
        "\n"
      ],
      "metadata": {
        "id": "p9mznUlehs7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def create_paths(base_path, dataset_type):\n",
        "    labeled_path = base_path + '/' + dataset_type\n",
        "\n",
        "    image_paths = []\n",
        "    label_paths = []\n",
        "\n",
        "    for class_folder in os.listdir(labeled_path):\n",
        "        if not class_folder.startswith('.'):  # Exclude hidden folders\n",
        "            class_path = labeled_path\n",
        "\n",
        "            if os.path.isdir(class_path) and class_folder == (dataset_type + '-org-img'):\n",
        "                org_images_path = class_path + '/' + class_folder\n",
        "\n",
        "                if os.path.isdir(org_images_path):\n",
        "                    image_paths += [org_images_path + '/' + filename for filename in os.listdir(org_images_path)]\n",
        "\n",
        "            if os.path.isdir(class_path) and class_folder == (dataset_type + '-label-img'):\n",
        "                labeled_images_path = class_path + '/' + class_folder\n",
        "\n",
        "                if os.path.isdir(labeled_images_path):\n",
        "                    label_paths += [labeled_images_path + '/' + filename for filename in os.listdir(labeled_images_path)]\n",
        "\n",
        "    return image_paths, label_paths\n",
        "\n",
        "base_dataset_path = '/Users/gokaytoga/Downloads/Augmented_Data'\n",
        "#/Downloads/Augmented_Data /Downloads/Augmented_Data\n",
        "#/content/drive/MyDrive/Augmented_Data\n",
        "\n",
        "# Create paths for training data\n",
        "x_train, y_train = create_paths(base_dataset_path, 'train')\n",
        "# Create paths for test data\n",
        "x_test, y_test = create_paths(base_dataset_path, 'test')\n",
        "# Create paths for val data\n",
        "x_val, y_val = create_paths(base_dataset_path, 'val')\n",
        "\n",
        "# Print the first few paths for verification\n",
        "print(x_train[:5])\n",
        "print(y_train[:5])\n",
        "\n",
        "\n",
        "batch_size = 4\n",
        "train_dataset = SegDataset3(x_train, y_train, batch_size=batch_size, transform = Aug_parameters)\n",
        "test_dataset = SegDataset3(x_test, y_test, batch_size=batch_size, transform=None)  # No augmentation for the test set\n",
        "val_dataset = SegDataset3(x_val, y_val, batch_size=batch_size, transform = Aug_parameters)\n",
        "\n",
        "#THIS PART IS FOR USE WHEN COMPUTING PERFORMANCE PROBLEMS OCCUR\n",
        "#target_size = (256, 256)\n",
        "#train_dataset = SegDataset3(x_train, y_train, batch_size=batch_size, transform = Aug_parameters, target_size=target_size)\n",
        "#test_dataset = SegDataset3(x_test, y_test, batch_size=batch_size, transform=None, target_size=target_size)  # No augmentation for the test set\n",
        "#val_dataset = SegDataset3(x_val, y_val, batch_size=batch_size, transform = Aug_parameters, target_size=target_size)\n",
        "\n",
        "\n",
        "image_datasets = {'train_set': train_dataset, 'test_set': test_dataset}\n",
        "#/Downloads/Augmented_Data/train\n",
        "#/Downloads/Augmented_Data/train\n",
        "#/Downloads/Augmented_Data/train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vjU9Ay3W21Y",
        "outputId": "845f92f2-643f-4114-c015-d19cce53e71b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/Users/gokaytoga/Downloads/Augmented_Data/train/train-org-img/7078.jpg', '/Users/gokaytoga/Downloads/Augmented_Data/train/train-org-img/6372.jpg', '/Users/gokaytoga/Downloads/Augmented_Data/train/train-org-img/6414.jpg', '/Users/gokaytoga/Downloads/Augmented_Data/train/train-org-img/7722.jpg', '/Users/gokaytoga/Downloads/Augmented_Data/train/train-org-img/8411.jpg']\n",
            "['/Users/gokaytoga/Downloads/Augmented_Data/train/train-label-img/7753_lab.png', '/Users/gokaytoga/Downloads/Augmented_Data/train/train-label-img/6564_lab.png', '/Users/gokaytoga/Downloads/Augmented_Data/train/train-label-img/6574_lab.png', '/Users/gokaytoga/Downloads/Augmented_Data/train/train-label-img/8962_lab.png', '/Users/gokaytoga/Downloads/Augmented_Data/train/train-label-img/7270_lab.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   The model:\n",
        "\n"
      ],
      "metadata": {
        "id": "qphS5pW_ldU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### UNET CONFIG #########\n",
        "ENCODER_DEPTH=5\n",
        "DECODER_CHANNELS=(256, 128, 64, 32, 16)\n",
        "BATCH_SIZE= [8, 16, 32, 64, 128]\n",
        "LR = [1, 1e-2, 1e-4, 1e-6]\n",
        "BATCH_SIZE = [8]\n",
        "LR = [1e-2]\n",
        "EPOCHS= 25"
      ],
      "metadata": {
        "id": "j1thZo2ou_Wz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(unique_name, num_epochs=EPOCHS, start_alpha_from=15, reach_max_alpha_in=655, max_alpha=0.5):\n",
        "    for lr in LR:\n",
        "        for bs in BATCH_SIZE:\n",
        "            print(\"__\" * 80)\n",
        "            print(\"__\" * 80)\n",
        "            print(f\"name: {unique_name} LR: {lr} BS: {bs}\")\n",
        "            print(\"__\" * 80)\n",
        "            print(\"__\" * 80)\n",
        "\n",
        "\n",
        "            # Create a model using TensorFlow/Keras\n",
        "            Backbone = tf.keras.applications.ResNet101 #create_tf_model()  # Implement your TensorFlow model creation function\n",
        "\n",
        "            # Compile the model with an optimizer and loss function\n",
        "            optimizer = SGD(learning_rate=lr)\n",
        "            model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "            # Create TensorBoard callback for logging\n",
        "            tensorboard_callback = TensorBoard(log_dir=f'logs/{unique_name}-{lr}-{bs}', histogram_freq=1)\n",
        "\n",
        "            # Define ModelCheckpoint callback for saving the best model\n",
        "            model_checkpoint = ModelCheckpoint(filepath=f'models/{unique_name}-best-model.h5', save_best_only=True)\n",
        "\n",
        "            # Training loop remains similar with TensorFlow-specific adjustments\n",
        "            for epoch in range(num_epochs):\n",
        "                # Rest of the training loop\n",
        "\n",
        "                # Use model.fit() for training instead of manual optimization steps\n",
        "                history = model.fit(train_dataset, epochs=1, callbacks=[tensorboard_callback, model_checkpoint])\n",
        "\n",
        "                # Rest of the training loop\n",
        "\n",
        "            # Save the final model\n",
        "            model.save(f'models/{unique_name}-final-model.h5')\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "QBVB0J2RtNpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tensorflow model:"
      ],
      "metadata": {
        "id": "caPQxlnXzUC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = tf.keras.applications.MobileNetV2(input_shape=[713, 713, 3], include_top=False)\n",
        "\n",
        "layer_names = [\n",
        "    'block_1_expand_relu',   # 64x64\n",
        "    'block_3_expand_relu',   # 32x32\n",
        "    'block_6_expand_relu',   # 16x16\n",
        "    'block_13_expand_relu',  # 8x8\n",
        "    'block_16_project',      # 4x4\n",
        "]\n",
        "base_model_outputs = [base_model.get_layer(name).output for name in layer_names]\n",
        "\n",
        "down_stack = tf.keras.Model(inputs=base_model.input, outputs=base_model_outputs)\n",
        "\n",
        "down_stack.trainable = False\n",
        "\n",
        "up_stack = [\n",
        "    pix2pix.upsample(512, 3),  # 4x4 -> 8x8\n",
        "    pix2pix.upsample(256, 3),  # 8x8 -> 16x16\n",
        "    pix2pix.upsample(128, 3),  # 16x16 -> 32x32\n",
        "    pix2pix.upsample(64, 3),   # 32x32 -> 64x64\n",
        "]\n",
        "\n",
        "def unet_model(output_channels:int):\n",
        "  inputs = tf.keras.layers.Input(shape=[DIMENSIONS, DIMENSIONS, 3])\n",
        "\n",
        "  # Downsampling through the model\n",
        "  skips = down_stack(inputs)\n",
        "  x = skips[-1]\n",
        "  skips = reversed(skips[:-1])\n",
        "\n",
        "  # Upsampling and establishing the skip connections\n",
        "  for up, skip in zip(up_stack, skips):\n",
        "    x = up(x)\n",
        "    concat = tf.keras.layers.Concatenate()\n",
        "    x = concat([x, skip])\n",
        "\n",
        "  # This is the last layer of the model\n",
        "  last = tf.keras.layers.Conv2DTranspose(\n",
        "      filters=output_channels, kernel_size=3, strides=2,\n",
        "      padding='same')  #64x64 -> 128x128\n",
        "\n",
        "  x = last(x)\n",
        "\n",
        "  return tf.keras.Model(inputs=inputs, outputs=x)"
      ],
      "metadata": {
        "id": "rksJ5G4Uw06-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "my Unet model:"
      ],
      "metadata": {
        "id": "rBC-sEIF0Qrg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResizeLayer(layers.Layer):\n",
        "    def __init__(self, target_shape, **kwargs):\n",
        "        super(ResizeLayer, self).__init__(**kwargs)\n",
        "        self.target_shape = target_shape\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.image.resize(inputs, self.target_shape)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(ResizeLayer, self).get_config()\n",
        "        config.update({'target_shape': self.target_shape})\n",
        "        return config\n",
        "\n",
        "def conv_block(x, filters, kernel_size, activation='relu'):\n",
        "    x = layers.Conv2D(filters, kernel_size, padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(activation)(x)\n",
        "    return x\n",
        "\n",
        "def unet_model(input_shape=(713, 713, 3), num_classes=10):\n",
        "    # Encoder\n",
        "    inputs = tf.keras.Input(shape=input_shape)\n",
        "    conv1 = conv_block(inputs, 64, 3)\n",
        "    conv1 = conv_block(conv1, 64, 3)\n",
        "    pool1 = layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "\n",
        "    conv2 = conv_block(pool1, 128, 3)\n",
        "    conv2 = conv_block(conv2, 128, 3)\n",
        "    pool2 = layers.MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "\n",
        "    conv3 = conv_block(pool2, 256, 3)\n",
        "    conv3 = conv_block(conv3, 256, 3)\n",
        "    pool3 = layers.MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "\n",
        "    # Bottleneck\n",
        "    conv4 = conv_block(pool3, 512, 3)\n",
        "    conv4 = conv_block(conv4, 512, 3)\n",
        "\n",
        "    # Decoder\n",
        "    up5 = layers.UpSampling2D(size=(2, 2))(conv4)\n",
        "    resize5 = ResizeLayer(target_shape=(conv3.shape[1], conv3.shape[2]))(up5)\n",
        "    concat5 = layers.Concatenate()([resize5, conv3])\n",
        "    conv5 = conv_block(concat5, 256, 3)\n",
        "    conv5 = conv_block(conv5, 256, 3)\n",
        "\n",
        "    up6 = layers.UpSampling2D(size=(2, 2))(conv5)\n",
        "    resize6 = ResizeLayer(target_shape=(conv2.shape[1], conv2.shape[2]))(up6)\n",
        "    concat6 = layers.Concatenate()([resize6, conv2])\n",
        "    conv6 = conv_block(concat6, 128, 3)\n",
        "    conv6 = conv_block(conv6, 128, 3)\n",
        "\n",
        "    up7 = layers.UpSampling2D(size=(2, 2))(conv6)\n",
        "    resize7 = ResizeLayer(target_shape=(conv1.shape[1], conv1.shape[2]))(up7)\n",
        "    concat7 = layers.Concatenate()([resize7, conv1])\n",
        "    conv7 = conv_block(concat7, 64, 3)\n",
        "    conv7 = conv_block(conv7, 64, 3)\n",
        "\n",
        "    # Output layer\n",
        "    output = layers.Conv2D(num_classes, 1, activation='softmax')(conv7)\n",
        "\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=output, name='unet_model')\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "McNJh-Q00VBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Training the model:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OkxFDvWGyzyL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OUTPUT_CLASSES = 10\n",
        "\n",
        "model = unet_model(output_channels=OUTPUT_CLASSES)\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['mIoU'])"
      ],
      "metadata": {
        "id": "z3-etwVOy3Vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = unet_model()\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='Adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=['accuracy'] )\n",
        "\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.ModelCheckpoint(filepath='model_checkpoint.h5', save_best_only=True),\n",
        "    tf.keras.callbacks.TensorBoard(log_dir='logs')\n",
        "]\n",
        "\n",
        "# Train the model\n",
        "#@tf.autograph.experimental.do_not_convert\n",
        "model.fit(train_dataset, epochs=EPOCHS, validation_data=val_dataset, callbacks=callbacks)"
      ],
      "metadata": {
        "id": "LDVWV1ay05mZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(test_dataset)"
      ],
      "metadata": {
        "id": "5qCKZO2x4INB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Visualization\n",
        "\n"
      ],
      "metadata": {
        "id": "mkHcesTJ4ZWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for image, true_mask in test_dataset.take(5):  # Visualize predictions for the first 5 samples\n",
        "    predicted_mask = model.predict(image[tf.newaxis, ...])[0]\n",
        "\n",
        "    # Visualize using matplotlib or other visualization libraries\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.imshow(image.numpy().astype('uint8'))\n",
        "    plt.title('Input Image')\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.imshow(true_mask.numpy().squeeze(), cmap='viridis', vmin=0, vmax=1)\n",
        "    plt.title('True Mask')\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.imshow(predicted_mask.argmax(axis=-1), cmap='viridis', vmin=0, vmax=1)\n",
        "    plt.title('Predicted Mask')\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "A2Gkc8Ub4ddq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}